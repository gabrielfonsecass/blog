---
title: "O futuro sem futuro da IA"
pubDate: 2025-03-13
description: "O risco da IA gerado por atores humanos deliberados (isto √©, concentra√ß√£o de poder) parece ser maior do que o risco de uso n√£o intencional (isto √©, perda de controle). "
author: "Gabriel Fonseca"
tags: ["AGI", "AI", "DeepLearning"]
---

## ü§ñ Reflex√µes sobre AGI

- **Defini√ß√£o interessante de AGI**: sistemas de IA que podem substituir completamente o trabalho humano (ou ter um impacto compar√°vel).
- O risco da IA gerado por atores humanos deliberados (isto √©, concentra√ß√£o de poder) parece ser maior do que o risco de uso n√£o intencional (isto √©, perda de controle). [Veja alguns dos riscos (muito bem analisados por sinal) [Aqui](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)]
  - **Observa√ß√£o**: o risco da AGI ainda pode ser alto - o uso malicioso pode facilmente ser pior do que o uso acidental.
  - A pesquisa sobre alinhamento parece ser tecnicamente mais interessante do que a pesquisa sobre seguran√ßa...
- Limita√ß√µes de dados (por exemplo, na medicina) podem restringir avan√ßos r√°pidos e generalizados.

---

## Compat√≠vel com Humanos

\**Notas baseadas no livro [*Compat√≠vel com Humanos\*](https://a.co/d/cTo8KGr), de Stuart Russel (2019)

### E se tivermos algum sucesso?

- Poss√≠veis maiores eventos do futuro da humanidade:
  - Todos morremos;
  - Todos vivemos para sempre;
  - Conquistamos o universo;
  - Somos visitados por uma civiliza√ß√£o alien√≠gena superior;
  - Inventamos uma IA superinteligente;
- _Defini√ß√£o_: Humanos s√£o inteligentes na medida em que suas a√ß√µes podem ser esperadas para atingir seus objetivos (dado o que percebem).
  - M√°quinas s√£o _ben√©ficas_ na medida em que _suas_ a√ß√µes podem ser esperadas para atingir _nossos_ objetivos.
- Efeito Baldwin - o aprendizado pode facilitar a evolu√ß√£o.
- A _utilidade_ para coisas como o dinheiro √© **decrescente**.
  - Agentes racionais maximizam a **utilidade esperada**.
- Jogn McCarthy ajudou a introduzir os _sistemas baseados em conhecimento_, que utilizam _l√≥gica de primeira ordem_.
  - No entanto, esses sistemas n√£o incorporavam incerteza.
  - A IA moderna usa utilidades e probabilidades em vez de objetivos e l√≥gica.
  - Redes bayesianas s√£o como l√≥gica proposicional probabil√≠sitica, combinadas com programa√ß√£o probabil√≠stica.
- A linguagem j√° codifica muito do que sabemos.
- _Programa√ß√£o l√≥gica indutiva_ - prop√µe novos conceitos e defini√ß√µes para identificar teorias que sejam precisas e concisas.
- Precisamos ser capazes de aprender muitas abstra√ß√µes √∫teis.
- Uma IA super-humana poderia realizar feitos incr√≠veis, como:
  - Guiar cada pessoa/ve√≠culo individualmente durante uma evacua√ß√£o;
  - Conduzir experimentos e comparar instantaneamente com todos os resultados existentes;
  - Meta de alto n√≠vel: elevar o padr√£o de vida de todos, em todos os lugares?
  - Tutoria baseada em IA.
- A regulamenta√ß√£o da UE (GDPR) - e a esquecida, abandonada e triste LGPD brasileira - sobre o "direito √† explica√ß√£o" √©, na verdade, mais fraca do que parece: exige apenas "informa√ß√µes significativas sobre a l√≥gica envolvida, bem como o significado e as consequ√™ncias previstas desse processamento para o titular dos dados".
- _whataboutery_ - t√©cnica para desviar quest√µes, onde algu√©m sempre responde "e o que voc√™ pensa de x?" em vez de engajar diretamente.

### Dados causados pela IA

- Exemplos:
  - Vigi√¢ncia, manipula√ß√£o e controle;
  - Armas aut√¥nomas letais (escal√°veis);
  - Extors√£o automatizada;
  - Deepfakes / m√≠dia falsa;
  - Automa√ß√£o - como resolver isso? Renda b√°sica universal (Eduardo Suplicy canta essa bola desde que meu av√¥ se conhece por gente)

### Alinhamento de valores

- Exemplos:
  - Rei Midas;
  - Dire√ß√£o perigosa;
  - Otimizar os n√≠veis de oxig√™nio no mar e acabar removendo o oxig√™nio do ar;
  - Encontrar a cura para o c√¢ncer e, acidentalmente, dar tumores a todos;
- Nota: para uma IA, pode ser mais f√°cil convencer os humanos de um objetivo diferente do que resolv√™-lo de fato.
- Praticamente qualquer objetivo de otimiza√ß√£o pode levar uma IA a desativar seu pr√≥prio bot√£o de desligamento.

### Poss√≠veis solu√ß√µes

- IA or√°culo - s√≥ pode responder perguntas de "sim/n√£o" ou probabil√≠sticas, sem interagir diretamente com o mundo real.
- Aprendizado por Refor√ßoo Inverso (Inverse RL):
  - A IA deve ter incerteza sobre as utilidades;
  - As utilidades devem ser inferidas a partir das prefer√™ncias humanas;
  - Em sistemas interativos, as prefer√™ncias devem ser expressas em termos de teoria dos jogos;
- Complica√ß√µes:
  - Dif√≠cil interpretar instru√ß√µes humanas corretamente;
  - Pessoas s√£o diferentes;
  - Uma IA leal a uma pessoa pode prejudicar outras;
  - √âtica da IA:
    - _Consequencialismo_ - as escolhas devem ser julgadas pelos resultados esperados;
    - _√âtica deontol√≥gica e √©tica das virtudes_ - preocupadas com o car√°ter moral das a√ß√µes e dos indiv√≠duos.
    - Dif√≠cil comparar utilidades entre pessoas;
    - O utilitarismo tem problemas quando envolve utilidades negativas;
  - Prefer√™ncias podem mudar;
- A IA deve ser regulada.
- O aprendizado profundo (deep learning) se assemelha muito aos nossos sistemas sensoriais - a l√≥gica ainda √© necess√°ria para agir sobre essas abstra√ß√µes.

---

## Mentes poss√≠veis

_Notas de [Mentes Poss√≠veis](https://a.co/d/akau7zz)_, uma colet√¢nea de ensaios organizada por Jogn Brockman (2019).

### Introdu√ß√£o (Brockman)

- Novas tecnologias = novas formas de perceber o mundo.
- Criamos ferramentas e nos moldamos por meio do seu uso.
- Norbert Wiener: "Devemos parar de beijar o chicote que nos a√ßoita".
  - Seu livro inicial foi _The Human Use of Human Beings_.
  - Ele era em sua maioria anal√≥gico, caiu em desuso.
  - Inicialmente inspirou o campo da IA.
- A IA j√° passou por ciclos de ascens√£o e queda.
- **GOFAI** - Good Old-Fashiones AI (IA cl√°ssica - ou "A boa e velha querida IA").
- Coisas que ach√°vamos dif√≠ceis, como xadrez, se mostraram f√°ceis para a IA.

### Errada, mas mais relevante do que nunca (Seth Lloyd)

- A IA atual √© muito pior do que as pessoas pensam que √©.
- Wiener era muito pessimista - WWWII / Guerra Fria.
- A singularidade n√£o est√° chegando...

### As limita√ß√µes das LMs opacas (Judea Pearl)

- 3 n√≠veis de racioc√≠nio:
  - estat√≠stico;
  - casual;
  - contrafactual - muitos contrafactuais, mas a linguagem √© boa e fornece muitos deles;
- "Explicar a aus√™ncia" = "bloqueio reverso" na literatura de condicionamento.
- Come√ßa com a infer√™ncia casual, mas n√£o funciona para sistemas grandes.
- Aprendizado profundo n√£o √© interpret√°vel.
- Exemplo: pergunte a algu√©m por que ela se divorciou?
  - renda, idade, etc...
  - algo sobre o relacionamento...
- Correla√ß√µes, causas, explica√ß√µes (morais/racionais) - biologicamente tendemos a isso?
  - cren√ßas + desejos causam a√ß√µes.
- √â interessante que diferentes pessoas sigam normal (por exemplo, chegar na aula √†s 08h).
  - voc√™ poderia fazer isso com IA?
- Exemplo de chatbot do Facebook.
- M√°quina de clipes de papel, an√∫ncios nas redes sociais.
- Estados/empresas s√£o como IAs.
- **Equifinidade** perturbe o comportamento (como usar imagens em escala de cinza em vez de coloridas) e eles ainda podem fazer isso (como a estabilidade)
