---
title: "Futuros da IA"
category: "Intelig√™ncia Artificial"
pubDate: 2025-03-13
description: "O risco da IA gerado por atores humanos deliberados (isto √©, concentra√ß√£o de poder) parece ser maior do que o risco de uso n√£o intencional (isto √©, perda de controle). "
author: "Gabriel Fonseca"
tags: ["AGI", "AI", "DeepLearning"]
---

## ü§ñ Reflex√µes sobre AGI 
- **Defini√ß√£o interessante de AGI**: sistemas de IA que podem substituir completamente o trabalho humano (ou ter um impacto compar√°vel).
- O risco da IA gerado por atores humanos deliberados (isto √©, concentra√ß√£o de poder) parece ser maior do que o risco de uso n√£o intencional (isto √©, perda de controle). [Veja alguns dos riscos (muito bem analisados por sinal) [Aqui](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)]
  - **Observa√ß√£o**: o risco da AGI ainda pode ser alto - o uso malicioso pode facilmente ser pior do que o uso acidental.
  - A pesquisa sobre alinhamento parece ser tecnicamente mais interessante do que a pesquisa sobre seguran√ßa...
- Limita√ß√µes de dados (por exemplo, na medicina) podem restringir avan√ßos r√°pidos e generalizados.

---

## Compat√≠vel com Humanos
\**Notas baseadas no livro [*Compat√≠vel com Humanos\*](https://a.co/d/cTo8KGr), de Stuart Russel (2019)

### E se tivermos algum sucesso?
- Poss√≠veis maiores eventos do futuro da humanidade:
  - Todos morremos;
  - Todos vivemos para sempre;
  - Conquistamos o universo;
  - Somos visitados por uma civiliza√ß√£o alien√≠gena superior;
  - Inventamos uma IA superinteligente;
- _Defini√ß√£o_: Humanos s√£o inteligentes na medida em que suas a√ß√µes podem ser esperadas para atingir seus objetivos (dado o que percebem).
  - M√°quinas s√£o _ben√©ficas_ na medida em que _suas_ a√ß√µes podem ser esperadas para atingir _nossos_ objetivos.
- Efeito Baldwin - o aprendizado pode facilitar a evolu√ß√£o.
- A _utilidade_ para coisas como o dinheiro √© **decrescente**.
  - Agentes racionais maximizam a **utilidade esperada**.
- Jogn McCarthy ajudou a introduzir os _sistemas baseados em conhecimento_, que utilizam _l√≥gica de primeira ordem_.
  - No entanto, esses sistemas n√£o incorporavam incerteza.
  - A IA moderna usa utilidades e probabilidades em vez de objetivos e l√≥gica.
  - Redes bayesianas s√£o como l√≥gica proposicional probabil√≠sitica, combinadas com programa√ß√£o probabil√≠stica.
- A linguagem j√° codifica muito do que sabemos.
- _Programa√ß√£o l√≥gica indutiva_ - prop√µe novos conceitos e defini√ß√µes para identificar teorias que sejam precisas e concisas.
- Precisamos ser capazes de aprender muitas abstra√ß√µes √∫teis.
- Uma IA super-humana poderia realizar feitos incr√≠veis, como:
  - Guiar cada pessoa/ve√≠culo individualmente durante uma evacua√ß√£o;
  - Conduzir experimentos e comparar instantaneamente com todos os resultados existentes;
  - Meta de alto n√≠vel: elevar o padr√£o de vida de todos, em todos os lugares?
  - Tutoria baseada em IA.
- A regulamenta√ß√£o da UE (GDPR) - e a esquecida, abandonada e triste LGPD brasileira - sobre o "direito √† explica√ß√£o" √©, na verdade, mais fraca do que parece: exige apenas "informa√ß√µes significativas sobre a l√≥gica envolvida, bem como o significado e as consequ√™ncias previstas desse processamento para o titular dos dados".
- _whataboutery_ - t√©cnica para desviar quest√µes, onde algu√©m sempre responde "e o que voc√™ pensa de x?" em vez de engajar diretamente.

### Dados causados pela IA
- Exemplos:
  - Vigi√¢ncia, manipula√ß√£o e controle;
  - Armas aut√¥nomas letais (escal√°veis);
  - Extors√£o automatizada;
  - Deepfakes / m√≠dia falsa;
  - Automa√ß√£o - como resolver isso? Renda b√°sica universal (Eduardo Suplicy canta essa bola desde que meu av√¥ se conhece por gente)

### Alinhamento de valores
- Exemplos:
  - Rei Midas;
  - Dire√ß√£o perigosa;
  - Otimizar os n√≠veis de oxig√™nio no mar e acabar removendo o oxig√™nio do ar;
  - Encontrar a cura para o c√¢ncer e, acidentalmente, dar tumores a todos;
- Nota: para uma IA, pode ser mais f√°cil convencer os humanos de um objetivo diferente do que resolv√™-lo de fato.
- Praticamente qualquer objetivo de otimiza√ß√£o pode levar uma IA a desativar seu pr√≥prio bot√£o de desligamento.

### Poss√≠veis solu√ß√µes
- IA or√°culo - s√≥ pode responder perguntas de "sim/n√£o" ou probabil√≠sticas, sem interagir diretamente com o mundo real.
- Aprendizado por Refor√ßoo Inverso (Inverse RL):
  - A IA deve ter incerteza sobre as utilidades;
  - As utilidades devem ser inferidas a partir das prefer√™ncias humanas;
  - Em sistemas interativos, as prefer√™ncias devem ser expressas em termos de teoria dos jogos;
- Complica√ß√µes:
  - Dif√≠cil interpretar instru√ß√µes humanas corretamente;
  - Pessoas s√£o diferentes;
  - Uma IA leal a uma pessoa pode prejudicar outras;
  - √âtica da IA:
    - _Consequencialismo_ - as escolhas devem ser julgadas pelos resultados esperados;
    - _√âtica deontol√≥gica e √©tica das virtudes_ - preocupadas com o car√°ter moral das a√ß√µes e dos indiv√≠duos.
    - Dif√≠cil comparar utilidades entre pessoas;
    - O utilitarismo tem problemas quando envolve utilidades negativas;
  - Prefer√™ncias podem mudar;
- A IA deve ser regulada.
- O aprendizado profundo (deep learning) se assemelha muito aos nossos sistemas sensoriais - a l√≥gica ainda √© necess√°ria para agir sobre essas abstra√ß√µes.

---

## Mentes poss√≠veis
_Notas de [Mentes Poss√≠veis](https://a.co/d/akau7zz)_, uma colet√¢nea de ensaios organizada por Jogn Brockman (2019).

### Introdu√ß√£o (Brockman)
- Novas tecnologias = novas formas de perceber o mundo.
- Criamos ferramentas e nos moldamos por meio do seu uso.
- Norbert Wiener: "Devemos parar de beijar o chicote que nos a√ßoita".
  - Seu livro inicial foi _The Human Use of Human Beings_.
  - Ele era em sua maioria anal√≥gico, caiu em desuso.
  - Inicialmente inspirou o campo da IA.
- A IA j√° passou por ciclos de ascens√£o e queda.
- **GOFAI** - Good Old-Fashiones AI (IA cl√°ssica - ou "A boa e velha querida IA").
- Coisas que ach√°vamos dif√≠ceis, como xadrez, se mostraram f√°ceis para a IA.

### Errada, mas mais relevante do que nunca (Seth Lloyd)
- A IA atual √© muito pior do que as pessoas pensam que √©.
- Wiener era muito pessimista - WWWII / Guerra Fria.
- A singularidade n√£o est√° chegando...

### As limita√ß√µes das LMs opacas (Judea Pearl)
- 3 n√≠veis de racioc√≠nio:
  - estat√≠stico;
  - casual;
  - contrafactual - muitos contrafactuais, mas a linguagem √© boa e fornece muitos deles;
- "Explicar a aus√™ncia" = "bloqueio reverso" na literatura de condicionamento.
- Come√ßa com a infer√™ncia casual, mas n√£o funciona para sistemas grandes.
- Aprendizado profundo n√£o √© interpret√°vel.
- Exemplo: pergunte a algu√©m por que ela se divorciou?
  - renda, idade, etc...
  - algo sobre o relacionamento...
- Correla√ß√µes, causas, explica√ß√µes (morais/racionais) - biologicamente tendemos a isso?
  - cren√ßas + desejos causam a√ß√µes.
- √â interessante que diferentes pessoas sigam normal (por exemplo, chegar na aula √†s 08h).
  - voc√™ poderia fazer isso com IA?
- Exemplo de chatbot do Facebook.
- M√°quina de clipes de papel, an√∫ncios nas redes sociais.
- Estados/empresas s√£o como IAs.
- **Equifinidade** perturbe o comportamento (como usar imagens em escala de cinza em vez de coloridas) e eles ainda podem fazer isso (como a estabilidade).

### O prop√≥sito colocado na m√°quina (Stuart Russell)
- Quer seguran√ßa na IA - precisa especificar o objetivo certo sem incertezas.
- **Alinhamento de valores** - coolocar o prop√≥sito certo.
- A pesquisa em IA estuda a capacidade de alcan√ßar objetivos, n√£o o design desses objetivos.
  - "melhor em tomar decis√µes - n√£o em tomar melhores decis√µes".
- Quer IA comprovadamente ben√©fica.
- N√£o pode apenas maximizar recompensas - a solu√ß√£o √≥tima √© controlar o humano para dar mais recompensas.
- RL inverso cooperativo - o rob√¥ aprende a fun√ß√£o de recompensa com o humano:
  - Assim, a incerteza sobre recompensas permite que o rob√¥ preserve seu bot√£o de desligamento;
  - A√ß√µes humanas nem sempre refletem suas verdadeiras prefer√™ncias.

### A terceira lei (George Dyson)
- 2 eras: antes e depois dos computadores digitais:
  - Antes: Thomas Hobbes, Gottfried Whilhelm Leibniz;
  - Depois:
    - Alan Turing - m√°quinas inteligentes;
    - Claude Shannon - comunica√ß√£o confi√°vel;
    - Norbert Wiener - quando as m√°quinas tomariam o controle;
- Computa√ß√£o anal√≥gica - tudo sobre corre√ß√µes de erros.
- A natureza usa cofidifica√ß√£o digital para prote√≠nas, mas anal√≥gica para o c√©rebro.
- Gr√°ficos sociais podem usar c√≥digo digital para computa√ß√£o anal√≥gica.
  - Sistemas anal√≥gicos parecem controlar o que est√£o mapeando (por exemplo, mapa de tr√°fego descentralizado)
- 3 leis da IA:
  - Lei de Ashby - qualquer sistema de controle eficaz deve ser t√£o complexo quanto o sistema que controla;
  - Lei de von Neumann - a caracter√≠stica definidora de um sistema complexo √© que ele constitui sua pr√≥pria descri√ß√£o comportamental mais simples;
  - 3¬™ lei - qualquer sistema simples o suficiente para ser compreendido n√£o ser√° complicado o suficiente para se comportar de maneira inteligente (e vice-versa).

### O que podemos fazer? (Daniel Dennett)
- Dennett escreveu *Das Bact√©rias a Bach e Vice-versa*.
- Elogios: disposi√ß√£o para admitir que est√° errado e manter a calma.
- Releitura das coisas abre novas portas.
- Importante tratar a IA como ferramenta - o verdadeiro perigo est√° nos humanos se tornarem escravos da IA que surgir√° naturalmente.
  - analogia com nossa depend√™ncia de frutas para vitamina C, enquanto outros animais sintetizam essa vitamina.
  - a tecnologia facilitou a manipula√ß√£o de evid√™ncias.
  - Wiener: "A longo prazo, n√£o h√° diferen√ß√£ entre armar-nos e armar nossos inimigos."
- A IA atual √© parasit√°ria da intelig√™ncia humana.
- Somos rob√¥s feitos de rob√¥s feitos de rob√¥s... sem ingredientes m√°gicos jogados no meio do caminho.
- Os embelezamentos human√≥ides atuais s√£o _propaganda enganosa_.
- Precisamos de uma maneira de testar a seguran√ßa/interpreta√ß√£o dos sistemas, talvez com elementos humanos.
- As pessoas automaticamente personificam as coisas.
- Precisamos de ferramentas inteligentes, n√£o consistentes - mais parecidas com or√°culos.
- √â muito dif√≠cil incorporar moralidade nas IAs - at√© a morte pode parecer boa.

### A unidade da intelig√™ncia (Frank Wilczek)
- Uma IA pode ser consciente/criativa/maldosa?
- A mente √© uma propriedade emergente da mat√©ria, isso implica que toda intelig√™ncia prov√©m de uma m√°quina.
- David Hume: "a raz√£o √©, e deve ser apenas, escrava das paix√µes".
- N√£o h√° uma divis√£o clara entre intelig√™ncia natural e artificial: parece que ambas operam na mesma f√≠sica.
- Intelig√™ncia parece ser um comportamento emergente.
- Principais diferen√ßas entre c√©rebros e computadores: c√©rebros podem se auto-reparar, t√™m maior conectividade, mas menor efici√™ncia no geral.
- A vantagem mais profunda do c√©rebro: conectividade e desenvolvimento interativo.
- As IAs ser√£o boas em explorar.
- Definir intelig√™ncia geral - talvez usando linguagem?
- O ambiente da Terra n√£o √© ideal para as IAs.
- A IA poderia controlar o mundo apenas com informa√ß√µes, n√£o apenas meios f√≠sicos.
- Economia afetiva - venda de emo√ß√µes (como conversar com o atendente do McDonalds).
- As pessoas parecem gostar de viver no mundo humano.
  - Ex: trabalhar em caf√©s, bibliotecas, etc.
- Future Life Institute - financiado por Elon... talvez s√≥ tentando ganhar dinheiro.

### Vamos aspirar a mais do que nos tornarmos obsoletos (Max Tegmark)
- √Äs vezes √© listado como alarmista.
- Talvez a consci√™ncia seja muito mais exagerada - como acordar de um estado de sonol√™ncia.
- Uma pesquisa com especialistas em IA apontou 50% de change da IA geral superar a intelig√™ncia humana at√© 2040-2050.
- Encontrar prop√≥sito se n√£o formos necess√°rios para nada?
- Import√¢ncia de manter a IA ben√©fica.
- As poss√≠veis IAs substituir√£o todos os empregos.
- Curiosidade √© perigosa.
- 3 raz√µes pelas quais o perigo da IA √© minimizado:
  1. As pessoas minimizam o perigo porque isso faz suas pesquisas parecerem boas - "√â dif√≠cil fazer um homem entender algo quando seu sal√°rio depende de ele n√£o entender" - Upton Sinclair.
    - **luddista** - pessoa oposta √† nova tecnologia ou maneiras de trabalhar - originado de uma organiza√ß√£o secreta de trabalhadores t√™xteis ingleses que protestavam.
  2. √â uma amea√ßa abstrata.
  3. Parece desesperador pensar sobre isso.
- A pesquisa de seguran√ßa da IA deve preceder os desenvolvimentos da IA.
- O verdadeiro risco com AGI n√£o √© a mal√≠cia, mas a compet√™ncia.
- Intelig√™ncia = capacidade de alcan√ßar objetivos complexos.
- Qu√£o bons as pessoas s√£o em prever o futuro da tecnologia?
- Joseph Weizenbaum excreveu sobre um bot psicoterapeuta que era bem ruim, mas o assustou.

### Mensagens Dissidentes (Jaan Tallinn)
- **Vozes que se levantam lentamente** acabam convencendo as pessoas.
- A **IA √© diferente** das tecnologias que vieram antes - ela pode se **auto-multiplicar**.
- O **c√©rebro humano** causou muitas mudan√ßas no mundo; a **IA ser√° similar**.
- As pessoas parecem estar mais inclinadas a reconhecer que o **risco √© grande**.
- **Riscos de curto prazo**: automa√ß√£o + vi√©s.
- **Um grande risco**: risco ambiental da IA: como restringir a IA para n√£o tornar nosso ambiente inabit√°vel para formas biol√≥gicas.
- Precisamos parar de ver o mundo como um **jogo de soma zero**.
- **Pesquisa famosa**: Katja Grace no Future of Humanity Institute.

### Profecia Tecnol√≥gica e o Poder Causal Subestimado das Ideias (Steven Pinker)
- "Assim como Darwin tornou poss√≠vel para um observador reflexivo do mundo natural viver sem criacionismo, Turing e outros tornaram poss√≠vel para um observador reflexivo do mundo cognitivo viver sem espiritualismo."
- **Vis√£o da entropia**: as IAs tentam evitar a entropia seguindo objetivos espec√≠ficos.
- As **ideias movem a hist√≥ria humana**.
- **2 poss√≠veis desfechos**:
  - Estado de vigil√¢ncia.
  - Reconhecimento autom√°tico de fala.
- **Pinker** acha que isso n√£o √© um grande problema porque a liberdade de pensamento √© movida por **normas e institui√ß√µes**, n√£o pela tecnologia.
- A maior amea√ßa da tecnologia parece ser amplificar **vozes duvidosas**, n√£o suprimir as iluminadas.
- Mais tecnologia tem se correlacionado com mais **democracia**.
- A IA toma o controle - parece muito com **determinismo tecnol√≥gico**.
- **Intelig√™ncia** √© a capacidade de implantar meios novos para alcan√ßar um objetivo - n√£o especifica qual √© o objetivo.
- **Conhecimento** s√£o coisas que sabemos - os nossos geralmente s√£o encontrar comida, parceiros, etc. As m√°quinas ter√£o **outros objetivos**.
- Se os humanos s√£o inteligentes o suficiente para criar IA, s√£o inteligentes o suficiente para **test√°-la**.
- "A amea√ßa n√£o √© a m√°quina, mas o que pode ser feito com ela."

### Al√©m de Recompensa e Puni√ß√£o (David Deutsch)
- **David Deutsch** - fundador da computa√ß√£o qu√¢ntica.
- Pensar envolve criar novas **hip√≥teses**, n√£o apenas ser bayesiano.
- O **conhecimento** em si n√£o era enormemente ben√©fico do ponto de vista evolutivo no come√ßo, mas reter conhecimento cultural era.
- No come√ßo, as pessoas n√£o aprendiam muito - apenas lembravam as normas culturais.
- Ningu√©m aspirava a nada novo.
- At√© agora, a maneira como as IAs foram desenvolvidas (por exemplo, jogar xadrez) √© restringir um **espa√ßo de busca**, mas AGI quer que elas criem um novo espa√ßo de busca.
- Geralmente, n√£o seguimos as leis por causa das puni√ß√µes - as AGIs tamb√©m n√£o v√£o.
- **A sociedade aberta** √© o √∫nico tipo est√°vel.
- Ser√° dif√≠cil **testar/otimizar diretamente**.
- AGI ainda pode ser **determinista**.
- **Tens√£o entre imita√ß√£o e aprendizado**? (imita√ß√£o/inova√ß√£o)
- As pessoas acreditam erroneamente que a AGI deveria aprender por conta pr√≥pria, como o causa sui de Nietzsche, mas os humanos n√£o fazem isso.
- **A cultura** pode te tornar mais livre de modelos.

### O Uso Artificial dos Seres Humanos (Tom Griffiths)
- Acredita que a chave para o **ML** √© o **aprendizado humano**.
- Agora temos bons modelos de **imagens/texto**, mas n√£o de **alinhamento de valores**.
- **RL inverso**: olhe para as a√ß√µes de um agente inteligente, aprenda a recompensa.
- **Precis√£o** (heur√≠sticas) vs **generaliza√ß√£o** (muitas vezes assume racionalidade).
- No entanto, as pessoas muitas vezes n√£o s√£o **racionais** - elas seguem **heur√≠sticas simples**.
  - Exemplo: n√£o calculam probabilidades, apenas tentam lembrar exemplos.
- As pessoas geralmente trocam **tempo** com a import√¢ncia de uma decis√£o - **otimalidade limitada**.
- A IA poderia realmente produzir mais **lazer**?

### Tornando o Invis√≠vel Vis√≠vel (Hans Ulrich Obrist)
- Precisamos usar a **arte** para interpretar melhor as visualiza√ß√µes, como o **deepdream**.
- **IA como ferramenta**, como o **photoshop**.
- Ajustar simula√ß√µes √© uma arte (novamente de maneira semelhante ao deep-dream).
- **Metas-meta** s√£o importantes.
- **Arte** - um sistema de alerta precoce para pensar sobre o futuro, evocativa.
- **Design** - tem um prop√≥sito mais claro, invis√≠vel.
- **Movimento fluxista** - fa√ßa voc√™ mesmo, como flash mob, espont√¢neo, n√£o elitista.
- Esta **exposi√ß√£o de progresso** - Guggenheim onde te entregam para as pessoas mais velhas.
- **Arte** - acompanha o que as pessoas apreciam ao longo do tempo.
- Tudo, exceto museus + pixels s√£o **pixels**.
- **Marcel Duchamp 1917** - urinol no museu de arte foi muito valioso.

### Sonho dos Algoritmistas de Objetividade (Peter Galison)
- Historiador da ci√™ncia.
- **Hist√≥rias sobre tecnologias perigosas** foram repetidas (por exemplo, nanotecnologia, DNA recombinante).
- Uma revis√£o em psicologia descobriu que **modelos objetivos** superaram grupos de cl√≠nicos humanos ("procedimentos de previs√£o: a controv√©rsia cl√≠nica-estat√≠stica").
- As pessoas come√ßaram inicialmente com **desenhos**.
- Depois mudaram para **medidas mais objetivas** (por exemplo, microsc√≥pio).
- Depois houve uma leve mudan√ßa para longe disso (por exemplo, humanos superaram algoritmos em certas coisas).
- **Objetividade n√£o √© tudo**.
- **Arte com sistema nervoso**.
- Anima√ß√µes com **personagens** que t√™m objetivos.

### Os Direitos das M√°quinas (George Church)
- As **m√°quinas devem**, cada vez mais, ter direitos como os dos humanos.
- **Potencial da IA** para tornar os humanos mais inteligentes tamb√©m.

### O Uso Art√≠stico de Seres Cibern√©ticos (Caroline Jones)
- Como **esticar** as pessoas al√©m de nossos par√¢metros simples e ego√≠stas.
- **Arte de s√©ance cibern√©tica**.
- Mais focado em **hardware**.
- Evolu√ß√£o baseada na **cultura**.
- **Vale do estranho** - se as coisas parecerem muito humanas, achamos assustadoras.
- Isso n√£o acontece com **crian√ßas** (at√© cerca de 10 anos).

### Reflex√µes sobre Arte com Base em Animais de Neil Mendoza
- A IA atual √© mais avan√ßada que o **jogo da vida**?

### Informa√ß√£o para Wiener, Shannon e para N√≥s (David Kaiser)
- **Wiener**: a sociedade s√≥ pode ser entendida com base na an√°lise de **mensagens**.
- **Informa√ß√£o** = informa√ß√£o sem√¢ntica.
- **Shannon**: **informa√ß√£o** = entropia (n√£o redu√ß√£o de entropia?).
- Previs√µes.
- A informa√ß√£o n√£o pode ser conservada (o n√≠vel eficaz da informa√ß√£o estar√° sempre avan√ßando).
- A informa√ß√£o n√£o √© adequada para ser **mercadoria**.
- Pode ser facilmente **replicada**.
- A ci√™ncia come√ßou a ter **cita√ß√µes** no s√©culo XVII porque antes disso as pessoas n√£o queriam publicar.
- Transformou a **informa√ß√£o** em moeda.
- O mundo da **arte** teve dificuldades com isso.
- Anos 80: **arte de apropria√ß√£o** - mudava apenas o t√≠tulo.
- A **literatura**, por muito tempo, n√£o tinha **direitos autorais**.
- **Algoritmos** s√£o dif√≠ceis de **patentear**.
- Aviso de **Wiener**: as m√°quinas nos dominariam apenas quando os **indiv√≠duos** fossem os mesmos.
- Estilo e outros aspectos se tornam mais semelhantes √† medida que estamos mais **conectados**.
- O **Twitter** seria o oposto disso.
- A **Amazon** poderia tornar as coisas mais homog√™neas.
- A **moda** muda constantemente.
- Talvez uma maneira arbitr√°ria de identificar **grupos** de dentro/fora.
- **Compara√ß√£o com mercados**.
- As **cidades** parecem aumentar a **diversidade** - mais pessoas para interagir.
- O **aprendizado profundo** deve buscar mais **informa√ß√£o sem√¢ntica** e n√£o **estat√≠stica**.


### Escalabilidade (Neil Gershenfield)
- A IA √© mais sobre **leis de escalabilidade** do que **modismos**.
- Mania: **sucesso em dom√≠nios limitados**.
- Depress√£o.